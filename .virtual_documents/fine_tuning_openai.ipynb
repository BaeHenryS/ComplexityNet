pip install openai -q


from openai import OpenAI
client = OpenAI(api_key="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")

file_create = client.files.create(
  file=open("mbpp_completion_460.jsonl", "rb"),
  purpose="fine-tune"
)



file_create


training_file = file_create.id
training_file


from openai import OpenAI


ft_create = client.fine_tuning.jobs.create(
  training_file=training_file, 
  model="davinci-002",
  hyperparameters={
    "n_epochs":4
  }
)


ft_create


ft_job = ft_create.id
ft_job


client.fine_tuning.jobs.retrieve(ft_job).fine_tuned_model


ft_model = client.fine_tuning.jobs.retrieve(ft_job).fine_tuned_model


response = client.completions.create(
  model=ft_model,
  prompt= "On a scale of 0 to 2 based solely on the complexity of creating the correct code for the task, where 0 represents a simpler task, 1 represents a moderately challenging task, and 2 represents a highly complex problem, the complexity of the task 'Write a function to find whether all the given tuples have equal length or not.' is: ",
  max_tokens=1
)
print(response.choices[0].text)


og_model = "davinci-002"


ft_model2 = "ft:davinci-002:personal::8OhPdNGc"


ft_model3 = client.fine_tuning.jobs.retrieve(ft_job).fine_tuned_model


import json

input_file_path = "./mbpp_completion_last_60.jsonl"

with open(input_file_path, 'r') as file:
    # Extract the 'prompt' field from each JSON object in the file and collect them into a list
    prompts = [json.loads(line)['prompt'] for line in file]

with open(input_file_path, 'r') as file:
    complexity = [json.loads(line)['completion'] for line in file]

prompts[-5:]


# og_model_res = []
# ft_model_res = []
ft_model_res2 = []
ft_model_res3 = []

for prompt in prompts:
    # og_response = client.completions.create(
    #               model=og_model,
    #               prompt= prompt,
    #               max_tokens=1)
    # og_model_res.append(int(og_response.choices[0].text))
    # ft_response = client.completions.create(
    #               model=ft_model,
    #               prompt= prompt,
    #               max_tokens=1)
    ft_response2 = client.completions.create(
                  model=ft_model2,
                  prompt= prompt,
                  max_tokens=1)
    ft_model_res2.append(int(ft_response2.choices[0].text))
    ft_response3 = client.completions.create(
                  model=ft_model3,
                  prompt= prompt,
                  max_tokens=1)
    ft_model_res3.append(int(ft_response3.choices[0].text))


len(complexity)


import pandas as pd


# Creating correctness columns
# og_correctness = [l1 == int(gt) for l1, gt in zip(og_model_res, complexity)]
# ft_correctness = [l2 == int(gt) for l2, gt in zip(ft_model_res, complexity)]
ft_correctness2 = [l3 == int(gt) for l3, gt in zip(ft_model_res2, complexity)]
ft_correctness3 = [l4 == int(gt) for l4, gt in zip(ft_model_res3, complexity)]

# Creating the DataFrame
df = pd.DataFrame({
    # 'prompt': prompts,
    # 'og': og_model_res,
    # 'og_correctness': og_correctness,
    # 'ft': ft_model_res,
    # 'ft_correctness': ft_correctness,
    'ft2': ft_model_res2,
    'ft_correctness2': ft_correctness2,
    'ft3': ft_model_res3,
    'ft_correctness3': ft_correctness3,
    'ground_truth': complexity,
})

df


ft_true_count2 = df['ft_correctness2'].sum()
# ft_true_count = df['ft_correctness'].sum()
ft_true_count3 = df['ft_correctness3'].sum()
ft_true_count3, ft_true_count2



