pip install pandas -q


import pandas as pd

# Replace 'your_file.jsonl' with your JSONL file path
file_path1 = './mbpp_label.jsonl'
file_path2 = './mbpp_label_llama.jsonl'

# Read the JSONL file
df1 = pd.read_json(file_path1, lines=True)
df2 = pd.read_json(file_path2, lines=True)
merged_df_corrected = pd.merge(df1, df2, on='task_id', how='outer', suffixes=('', '_df2'))

# We now remove the duplicate columns from df2 that are already present in df1 except those starting with 'method2'
columns_to_drop = [col for col in merged_df_corrected if col.endswith('_df2') and not col.startswith('method2')]
merged_df_corrected.drop(columns=columns_to_drop, inplace=True)

# Renaming the columns to remove the '_df2' suffix from the 'method2' columns from df2
merged_df_corrected.rename(columns=lambda x: x.replace('_df2', ''), inplace=True)
merged_df_corrected.head()
df = merged_df_corrected
df.head()


# List of columns to delete, replace these with your column names
columns_to_delete = ['code', 'task_id', 'test_setup_code', 'test_list', 'challenge_test_list', 'method2_gpt4_output', 'method2_gpt3_5_output', 'method2_llama_output']

# Delete the columns
df.drop(columns=columns_to_delete, inplace=True)


df.head()


# Update the function to incorporate the new condition
# def determine_value_updated(row):
#     if row['method2_llama_success']:
#         return 0
#     elif row['method2_gpt3_5_success']:
#         return 1
#     elif row['method2_gpt4_success']:
#         return 2
#     elif not row['method2_gpt4_success'] and not row['method2_gpt3_5_success']:
#         return 3
#     return None  # This case should not occur based on the provided conditions

# # Apply the updated function to the DataFrame
# df['complexity'] = df.apply(determine_value_updated, axis=1)

# # Show the updated DataFrame
# df


# Case of merging last two classes
def determine_value_updated(row):
    if row['method2_llama_success']:
        return 0
    elif row['method2_gpt3_5_success']:
        return 1
    else:
        return 2

# Apply the updated function to the DataFrame
df['complexity'] = df.apply(determine_value_updated, axis=1)

# Show the updated DataFrame
df


columns_to_delete = ['method2_gpt4_success', 'method2_gpt3_5_success', 'method2_llama_success']

# Delete the columns
df.drop(columns=columns_to_delete, inplace=True)


df_train = df[:400]
df_test = df[400:]


import matplotlib.pyplot as plt

class_counts = df['complexity'].value_counts()
plt.bar(class_counts.index, class_counts.values)


output_file_path_train = './mbpp_complexity_3classes_train.jsonl'
output_file_path_test = './mbpp_complexity_3classes_test.jsonl'
df_train.to_json(output_file_path_train, orient='records', lines=True)
df_test.to_json(output_file_path_test, orient='records', lines=True)


# For a chat model

import json

# Let's assume we read this data from 'current_dataset.jsonl'
input_file_path = './mbpp_complexity_train.jsonl'  # Replace with your actual file path
output_file_path = './mbpp_chat_train.jsonl'

# Read the JSONL file
with open(input_file_path, 'r') as file:
    current_data = [json.loads(line) for line in file]

# Define the system message that will be constant for all entries
system_message = {
    "role": "system",
    "content": "Output 0, 1, 2, or 3 based solely on the difficulty of creating the correct code for the task. "
                "Choose the least complex model that will solve the task accurately; avoid considering the time "
                "taken for solving. 0 represents a simpler task, 1 represents a slightly challenging task, 2 "
                "represents a moderately challenging task, and 3 represents a highly complex problem."
}

# Transform the current data to the desired format
transformed_data = []

for item in current_data:
    user_message = {
        "role": "user",
        "content": item["text"]
    }
    assistant_message = {
        "role": "assistant",
        "content": str(item["complexity"])
    }
    message_block = {
        "messages": [system_message, user_message, assistant_message]
    }
    transformed_data.append(message_block)

# Write the transformed data to a new JSONL file
with open(output_file_path, 'w') as outfile:
    for entry in transformed_data:
        json_record = json.dumps(entry)
        outfile.write(f"{json_record}\n")

output_file_path


# For a chat model

import json

# Let's assume we read this data from 'current_dataset.jsonl'
input_file_path = './mbpp_complexity_test.jsonl'  # Replace with your actual file path
output_file_path = './mbpp_chat_test.jsonl'

# Read the JSONL file
with open(input_file_path, 'r') as file:
    current_data = [json.loads(line) for line in file]

# Define the system message that will be constant for all entries
system_message = {
    "role": "system",
    "content": "First, think step by step about the problem. Only after thinking thoroughly about the problem and showing your thinking process, output 0, 1, 2, or 3 based solely on the difficulty of creating the correct code for the task. "
                "Choose the least complex model that will solve the task accurately; avoid considering the time "
                "taken for solving. 0 represents a simpler task, 1 represents a slightly challenging task, 2 "
                "represents a moderately challenging task, and 3 represents a highly complex problem. Think step by step before outputting a number then output the number."
}

# Transform the current data to the desired format
transformed_data = []

for item in current_data:
    user_message = {
        "role": "user",
        "content": item["text"]
    }
    assistant_message = {
        "role": "assistant",
        "content": str(item["complexity"])
    }
    message_block = {
        "messages": [system_message, user_message, assistant_message]
    }
    transformed_data.append(message_block)

# Write the transformed data to a new JSONL file
with open(output_file_path, 'w') as outfile:
    for entry in transformed_data:
        json_record = json.dumps(entry)
        outfile.write(f"{json_record}\n")

output_file_path


data_path = "./mbpp_chat_train.jsonl"

# Load the dataset
with open(data_path, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

# Initial dataset stats
print("Num examples:", len(dataset))
print("First example:")
for message in dataset[0]["messages"]:
    print(message)


pip install tiktoken -q


pip install collection -q


import tiktoken # for token counting
import numpy as np
from collections import defaultdict
# Format error checks
format_errors = defaultdict(int)

for ex in dataset:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue
        
    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue
        
    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1
        
        if any(k not in ("role", "content", "name", "function_call") for k in message):
            format_errors["message_unrecognized_key"] += 1
        
        if message.get("role", None) not in ("system", "user", "assistant", "function"):
            format_errors["unrecognized_role"] += 1
            
        content = message.get("content", None)
        function_call = message.get("function_call", None)
        
        if (not content and not function_call) or not isinstance(content, str):
            format_errors["missing_content"] += 1
    
    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")


encoding = tiktoken.get_encoding("cl100k_base")

# not exact!
# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

def num_assistant_tokens_from_messages(messages):
    num_tokens = 0
    for message in messages:
        if message["role"] == "assistant":
            num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

def print_distribution(values, name):
    print(f"\n#### Distribution of {name}:")
    print(f"min / max: {min(values)}, {max(values)}")
    print(f"mean / median: {np.mean(values)}, {np.median(values)}")
    print(f"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}")


# Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

for ex in dataset:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))
    
print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l > 4096 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning")


# Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 4096

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

n_epochs = TARGET_EPOCHS
n_train_examples = len(dataset)
if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")


tokens = n_epochs * n_billing_tokens_in_dataset


cost_dvnci = n_epochs * n_billing_tokens_in_dataset * 0.001 * 0.006
cost_dvnci



