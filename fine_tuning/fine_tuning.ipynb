{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuEFFGB59JUp"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"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\")"
      ],
      "metadata": {
        "id": "3NPzwJDx9Ndp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_create = client.files.create(\n",
        "  file=open(\"5class_train.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "val_create = client.files.create(\n",
        "  file=open(\"5class_val.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "uueQfzJ09NwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_file = train_create.id\n",
        "val_file = val_create.id\n",
        "training_file, val_file"
      ],
      "metadata": {
        "id": "DuX991y99N9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_create = client.fine_tuning.jobs.create(\n",
        "  training_file=training_file,\n",
        "  model=\"davinci-002\",\n",
        "  validation_file=val_file,\n",
        "  hyperparameters={\n",
        "    \"n_epochs\":4\n",
        "  }\n",
        ")"
      ],
      "metadata": {
        "id": "y5PYmHyp9OZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_create"
      ],
      "metadata": {
        "id": "Nol2hz9b9Om1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_job = ft_create.id\n",
        "ft_job"
      ],
      "metadata": {
        "id": "nd-7u_h29O3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.retrieve(ft_job)"
      ],
      "metadata": {
        "id": "xzoREYe29PEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = client.fine_tuning.jobs.retrieve(ft_job).fine_tuned_model"
      ],
      "metadata": {
        "id": "vOjIGEg29YD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model"
      ],
      "metadata": {
        "id": "4jJBtoRj9YPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_model = \"davinci-002\""
      ],
      "metadata": {
        "id": "esKxkV-I9YqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleury_174_NEW_1 = \"ft:davinci-002:personal::8U30Rpcm\""
      ],
      "metadata": {
        "id": "D0lOmxDe9Y1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleury_174_NEW_2 = \"ft:davinci-002:personal::8U3Wv6wf\""
      ],
      "metadata": {
        "id": "DbEH0Vzp9ZBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleury_174_NEW_3 = \"ft:davinci-002:personal::8U3ip1tK\""
      ],
      "metadata": {
        "id": "DXIUuj-B9ZNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleury_174_NEW_4 = \"ft:davinci-002:personal::8U4Hps37\""
      ],
      "metadata": {
        "id": "i1CAnrSZ9hQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleury_174_NEW_5 = \"ft:davinci-002:personal::8U6hkoKq\""
      ],
      "metadata": {
        "id": "q_f11CJh9hjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"./5class_test.jsonl\"\n",
        "\n",
        "with open(input_file_path, 'r') as file:\n",
        "    # Extract the 'prompt' field from each JSON object in the file and collect them into a list\n",
        "    prompts = [json.loads(line)['prompt'] for line in file]\n",
        "\n",
        "with open(input_file_path, 'r') as file:\n",
        "    complexity = [json.loads(line)['completion'] for line in file]"
      ],
      "metadata": {
        "id": "vuEg7LwN9hwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the Fine-Tuned Models (remove the for loops single API calls)\n",
        "og_model_res = []\n",
        "ft_model_res = []\n",
        "\n",
        "for prompt in prompts:\n",
        "    ft_arr = []\n",
        "    for _ in range(5):\n",
        "\n",
        "      ft_response = client.completions.create(\n",
        "                      model=fleury_174_NEW_5,\n",
        "                      prompt=prompt,\n",
        "                      max_tokens=1,\n",
        "                      logprobs=100  # Requesting log probabilities for better insight\n",
        "      )\n",
        "      ft_arr.append({\n",
        "          \"text\": prompt,\n",
        "          \"response\": int(ft_response.choices[0].text),\n",
        "          \"logprobs\": ft_response.choices[0].logprobs   # Accessing log probabilities\n",
        "      })\n",
        "    ft_model_res.append(ft_arr)\n",
        "\n",
        "    og_arr = []\n",
        "    for _ in range(5):\n",
        "      og_dav_response = client.completions.create(\n",
        "                          model=og_model,\n",
        "                          prompt=prompt,\n",
        "                          max_tokens=1,\n",
        "                          logprobs=100  # Requesting log probabilities\n",
        "      )\n",
        "      og_arr.append({\n",
        "          \"text\": prompt,\n",
        "          \"response\": int(og_dav_response.choices[0].text),\n",
        "          \"logprobs\": og_dav_response.choices[0].logprobs  # Accessing log probabilities\n",
        "      })\n",
        "    og_model_res.append(og_arr)\n"
      ],
      "metadata": {
        "id": "w9eteW5i9h8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Creating correctness columns\n",
        "# og_correctness = [l1['response'] == int(gt) or l1['response'] == int(gt) - 1 or l1['response'] == int(gt) + 1 for l1, gt in zip(og_model_dav3_res, complexity)]\n",
        "og_responses =[[item['response'] for item in l1] for l1, gt in zip(og_model_res, complexity)]\n",
        "# ft_correctness_true = [l2['response'] == int(gt) for l2, gt in zip(ft_model_res, complexity)]\n",
        "# ft_correctness = [l2['response'] == int(gt) or l2['response'] == int(gt) - 1 or l2['response'] == int(gt) + 1 for l2, gt in zip(ft_model_res, complexity)]\n",
        "responses = [[item['response'] for item in l2] for l2, gt in zip(ft_model_res, complexity)]\n",
        "\n",
        "# Creating the DataFrame\n",
        "df = pd.DataFrame({\n",
        "    # 'prompt': prompts,\n",
        "    'complexity': complexity,\n",
        "    'fine_tuned_responses': responses,\n",
        "    'og_responses': og_responses,\n",
        "    # 'og_correctness': og_correctness,\n",
        "    # 'ft_correctness': ft_correctness\n",
        "    # 'og': og_model_res,\n",
        "    # 'ft_correctness_true': ft_correctness_true,\n",
        "    # 'ft_correctness': ft_correctness,\n",
        "})\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "-Cz0kv2f9iJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the results of correctness for the dataset\n",
        "og_true_count = df['og_correctness'].sum()\n",
        "ft_true_count = df['ft_correctness'].sum()\n",
        "# ft_count_actual = df['ft_correctness_true'].sum()\n",
        "# greater = df['ft_greater'].sum()\n",
        "print(og_true_count, ft_true_count)"
      ],
      "metadata": {
        "id": "wdxrLqm89iXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(model, complexity_arr):\n",
        "  arr = []\n",
        "  for l1, gt in zip(model, complexity_arr):\n",
        "    i = 0\n",
        "    for elt in l1['logprobs']:\n",
        "      i += 1\n",
        "      if i != 4:\n",
        "        continue\n",
        "      else:\n",
        "        i = 0\n",
        "        try:\n",
        "          arr.append(elt[1][0][str(int(gt))])\n",
        "        except:\n",
        "          continue\n",
        "  return sum(arr) / len(arr), arr\n",
        "print(get_loss(ft_model_res, complexity))"
      ],
      "metadata": {
        "id": "TYTpA0Wy9ij6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}