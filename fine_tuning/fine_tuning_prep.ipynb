{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Define the complexity function\n",
        "def get_complexity(obj):\n",
        "    llama_num = obj['code_llama_success']\n",
        "    gpt3_num = obj['method2_gpt3_5_success']\n",
        "    gpt4_num = obj['method2_gpt4_success']\n",
        "    sum = llama_num + gpt3_num + gpt4_num\n",
        "    if (llama_num + gpt3_num >= 7) or llama_num == 5:\n",
        "        return 1\n",
        "    elif gpt3_num == 5:\n",
        "        return 2\n",
        "    elif gpt4_num == 5:\n",
        "        return 3\n",
        "    elif gpt4_num >= 2 or gpt3_num == 2:\n",
        "        return 4\n",
        "    else:\n",
        "      return 5"
      ],
      "metadata": {
        "id": "jC9sp6kIAFa1"
      },
      "id": "jC9sp6kIAFa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine data from Code Llama and GPT 3.5 and GPT 4"
      ],
      "metadata": {
        "id": "bP7RICGCAqqI"
      },
      "id": "bP7RICGCAqqI"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Paths to the input files\n",
        "file2_path = \"./mbpp_label_refined_gpt3_4_238.jsonl\"\n",
        "file1_path = \"./mbpp_label_refined_llama238.jsonl\"\n",
        "file3_path = \"./mbpp_test_5class_completion.jsonl\"\n",
        "\n",
        "# Open both files for reading and writing\n",
        "with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "    # Read lines from both files\n",
        "    lines_file1 = file1.readlines()\n",
        "    lines_file2 = file2.readlines()\n",
        "\n",
        "    # Iterate through each line in both files\n",
        "    for index, line_file1 in enumerate(lines_file1):\n",
        "        # Extract the 'code_llama_success' property from file1\n",
        "        data = json.loads(line_file1)\n",
        "        code_llama_success = data.get('code_llama_success')\n",
        "\n",
        "        # Update the corresponding line in file2 with 'code_llama_success' from file1\n",
        "        data_file2 = json.loads(lines_file2[index])\n",
        "        data_file2['code_llama_success'] = code_llama_success\n",
        "        lines_file2[index] = json.dumps(data_file2) + '\\n'\n",
        "\n",
        "# Write the updated content back to file2\n",
        "with open(file3_path, 'w') as file2:\n",
        "    file2.writelines(lines_file2)"
      ],
      "metadata": {
        "id": "UOZ0FT48AXJN"
      },
      "id": "UOZ0FT48AXJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove the noise - all data points with GPT-4, GPT-3.5, and Code Llama succeeding 0 times"
      ],
      "metadata": {
        "id": "qptMQkFPAjaq"
      },
      "id": "qptMQkFPAjaq"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to the input JSONL file\n",
        "input_file_path = \"./mbpp_label_refined_all.jsonl\"\n",
        "output_file_path = \"./mbpp_label_refined_no_zero.jsonl\"\n",
        "\n",
        "# Read the input file and filter rows\n",
        "with open(input_file_path, 'r') as file:\n",
        "    # Read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # Filter rows where all three keys have a value of 0\n",
        "    filtered_lines = [line for line in lines if json.loads(line).get('code_llama_success', 0) != 0\n",
        "                      or json.loads(line).get('method2_gpt3_5_success', 0) != 0\n",
        "                      or json.loads(line).get('method2_gpt4_success', 0) != 0]\n",
        "\n",
        "# Write the filtered content back to the file\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.writelines(filtered_lines)"
      ],
      "metadata": {
        "id": "8_AkFuAqAafh"
      },
      "id": "8_AkFuAqAafh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Path to the input JSONL file\n",
        "input_file_path = \"./mbpp_label_no_zero.jsonl\"\n",
        "file3_path = \"./mbpp_label_refined_no_zero_complexity.jsonl\"\n",
        "\n",
        "# Read the input file and process each line\n",
        "with open(input_file_path, 'r') as file:\n",
        "    # Read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # Process each line\n",
        "    for index, line in enumerate(lines):\n",
        "        # Load JSON object from the line\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Append 'complexity' key with the associated value\n",
        "        data['complexity'] = get_complexity(data)\n",
        "\n",
        "        # Update the line with the appended 'complexity' key\n",
        "        lines[index] = json.dumps(data) + '\\n'\n",
        "\n",
        "# Write the updated content back to the file\n",
        "with open(file3_path, 'w') as file:\n",
        "    file.writelines(lines)"
      ],
      "metadata": {
        "id": "4Bl65XEM_n2R"
      },
      "id": "4Bl65XEM_n2R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the data for OpenAI fine-tuning"
      ],
      "metadata": {
        "id": "ydGJuf6aAdFZ"
      },
      "id": "ydGJuf6aAdFZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file_path = './mbpp_label_refined_no_zero_complexity.jsonl'  # Path to the provided JSONL file\n",
        "output_file_path = './mbpp_test_5class_completion.jsonl'  # Path for the output JSONL file\n",
        "\n",
        "# Read the JSONL file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    current_data = [json.loads(line) for line in file]\n",
        "\n",
        "# Perform the transformation\n",
        "transformed_data = []\n",
        "\n",
        "for item in current_data:\n",
        "    transformed_entry = {\n",
        "        \"prompt\": f\"On a scale of 1 to 5 based solely on the complexity of creating the correct code for the task, \"\n",
        "                  f\"where 1 represents a very easy task, 2 represents a fairly straightforward task, and 3 represents \"\n",
        "                  f\"a medium difficulty task, 4 represents a challenging task, and 5 represents \"\n",
        "                  f\"a highly complex problem, the complexity of the task \\\"{item['text']}\\\" is: \",\n",
        "        \"completion\": str(item[\"complexity\"])\n",
        "    }\n",
        "    transformed_data.append(transformed_entry)\n",
        "\n",
        "# Write the transformed data to a new JSONL file\n",
        "with open(output_file_path, 'w') as outfile:\n",
        "    for entry in transformed_data:\n",
        "        json_record = json.dumps(entry)\n",
        "        outfile.write(f\"{json_record}\\n\")"
      ],
      "metadata": {
        "id": "g3rjER5LAClk"
      },
      "id": "g3rjER5LAClk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we output the test-train-validation files for the fine-tuning. To be used in the fine_tuning.py file"
      ],
      "metadata": {
        "id": "Ju_ykFnJ_xx2"
      },
      "id": "Ju_ykFnJ_xx2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths for output files\n",
        "output_file_1 = \"./5class_train.jsonl\"\n",
        "output_file_2 = \"./5class_val.jsonl\"\n",
        "output_file_3 = \"./5class_test.jsonl\"\n",
        "\n",
        "# Path to the input JSONL file\n",
        "input_file_path = \"./mbpp_test_5class_completion.jsonl\"\n",
        "\n",
        "# Read the input file and divide into sections\n",
        "with open(input_file_path, 'r') as file:\n",
        "    # Read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # test-train-validation split #1\n",
        "    # section_1 = lines[:120]\n",
        "    # section_2 = lines[120:140]\n",
        "    # section_3 = lines[140:174]\n",
        "\n",
        "    # test-train-validation split #2\n",
        "    # section_1 = lines[170:] + lines[10:40] + lines[60:90] + lines[100:150] + lines[154:160]\n",
        "    # section_2 = lines[90:100] + lines[160:170]\n",
        "    # section_3 = lines[40:60] + lines[0:10] + lines[150:154]\n",
        "\n",
        "    # test-train-validation split #3\n",
        "    # section_1 = lines[:80] + lines[134:]\n",
        "    # section_2 = lines[80:100]\n",
        "    # section_3 = lines[100:134]\n",
        "\n",
        "    # test-train-validation split #4\n",
        "    # section_1 = lines[0:116] + lines[150:154]\n",
        "    # section_2 = lines[154:174]\n",
        "    # section_3 = lines[116:150]\n",
        "\n",
        "    # # line breakup 5\n",
        "    section_1 = lines[170:] + lines[10:40] + lines[50:60] + lines[70:100] + lines[110:150] + lines[154:160]\n",
        "    section_2 = lines[100:110] + lines[160:170]\n",
        "    section_3 = lines[40:50] + lines[60:70] + lines[0:10] + lines[150:154]\n",
        "\n",
        "\n",
        "# Write each section to different output files\n",
        "with open(output_file_1, 'w') as file:\n",
        "    file.writelines(section_1)\n",
        "\n",
        "with open(output_file_2, 'w') as file:\n",
        "    file.writelines(section_2)\n",
        "\n",
        "with open(output_file_3, 'w') as file:\n",
        "    file.writelines(section_3)\n"
      ],
      "metadata": {
        "id": "qbveHNMU_wBl"
      },
      "id": "qbveHNMU_wBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are functions that were not used in the generation / testing of data but were used in the analysis of the distribution of complexity in the dataset."
      ],
      "metadata": {
        "id": "4_cCG0Od_mc1"
      },
      "id": "4_cCG0Od_mc1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to show distribution of complexity graphically in dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data (replace this with your array of integers)\n",
        "data = global_arr  # Replace this with your array of integers\n",
        "\n",
        "# Create a histogram\n",
        "plt.hist(data, bins=10, alpha=0.7, color='blue')\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Integer Data')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate summary statistics\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "std_dev = np.std(data)\n",
        "min_val = np.min(data)\n",
        "max_val = np.max(data)\n",
        "mode_val = stats.mode(data)\n",
        "\n",
        "twenty = np.percentile(data, 20)\n",
        "forty = np.percentile(data, 40)\n",
        "seventy = np.percentile(data, 70)\n",
        "\n",
        "def get_quartiles(arr):\n",
        "  return twenty, forty, seventy\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Median: {median}\")\n",
        "print(f\"Mode: {mode_val}\")\n",
        "print(f\"Standard Deviation: {std_dev}\")\n",
        "print(f\"Minimum Value: {min_val}\")\n",
        "print(f\"Maximum Value: {max_val}\")\n",
        "print(f\" twenty: {twenty}\")\n",
        "print(f\"forty {forty}\")\n",
        "print(f\"seventy {seventy}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s728escDFB6v"
      },
      "id": "s728escDFB6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Count Occurrences of Complexity in Dataset\n",
        "import json\n",
        "\n",
        "def count_complexity_occurrences(file_path):\n",
        "    complexity_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            complexity = data.get('completion')\n",
        "            # Check if 'complexity' exists and is in the range 1-5\n",
        "            if complexity is not None and int(complexity) in range(1, 6):\n",
        "                complexity_counts[int(complexity)] += 1\n",
        "\n",
        "    return complexity_counts\n",
        "\n",
        "# Replace 'file_path.jsonl' with the path to your JSONL file\n",
        "result = count_complexity_occurrences('mbpp_test_5class_completion.jsonl')\n",
        "\n",
        "print(\"Complexity Counts:\")\n",
        "for complexity, count in result.items():\n",
        "    print(f\"Complexity {complexity}: {count} occurrences\")\n"
      ],
      "metadata": {
        "id": "q322nA-pyHA7"
      },
      "id": "q322nA-pyHA7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}